# Import necessary libraries
from datasets import load_dataset  # Import function to load datasets
from transformers import GPT2Tokenizer, GPT2ForSequenceClassification  # Import GPT2 model and tokenizer
import evaluate  # Custom evaluation script
import numpy as np  # Import numpy library for numerical operations
from transformers import Trainer, TrainingArguments

# For more information check: https://drlee.io/fine-tuning-gpt-2-for-sentiment-analysis-94ebdd7b5b24

# Load the dataset for sentiment analysis from Twitter
dataset = load_dataset("mteb/tweet_sentiment_extraction")
#
# # Initialize GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Set padding token to end-of-sequence token
# This line ensures that when we pad sequences to make them all the same length,
# we use the end-of-sequence token (eos_token) as the padding token.
# It's like adding blank spaces at the end of a sentence to make it as long as the longest sentence.
tokenizer.pad_token = tokenizer.eos_token
#
# Tokenization function to preprocess the text data
def tokenize_function(examples):
    """
   This function takes a bunch of sentences and turns them into numbers that a computer can understand.
   It adds extra numbers to make all the sentences the same length, like making sure all the trains have the same number of carriages.

   Example:
       Input:
           examples = {"text": ["This is an example sentence.", "Another example sentence."]}
       Output:
           {'input_ids': [[1217, 318, 281, 2223, 5958, 13, 50256, 0, 0, 0], [1723, 281, 2223, 5958, 13, 50256, 0, 0, 0, 0]],
            'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]}

    The output of this function is a special set of lists (input_ids) for each sentence, along with another list (attention_mask)
    that tells the computer which parts of the sentence to focus on.
    It's like giving the computer a map so it knows where to look.
    """
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Apply tokenization function to the entire dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)
#
# Prepare smaller training and evaluation datasets for faster training
# We're selecting a smaller subset of the training and evaluation datasets to work with.
# This makes training faster because we're using fewer examples to teach the model.
# Think of it like studying a smaller set of homework problems instead of the entire textbook.
# when we shuffle the data and select a random subset, we ensure that the model gets exposed to
# a diverse range of examples, which can help improve its overall performance.
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

# Initialize GPT-2 model for sequence classification with 3 labels (positive, negative, neutral)
# We're setting up the GPT-2 model to learn how to classify sequences of text into one of three categories:
# positive, negative, or neutral sentiment.
# The GPT-2 model is like a student, and we're telling it to specialize in understanding and categorizing
# different types of text sequences.
# By specifying num_labels=3, we're telling the model that there are three possible categories
# it needs to learn to classify text into.
model = GPT2ForSequenceClassification.from_pretrained("gpt2", num_labels=3)
#
# Load evaluation metric (accuracy)
# We're setting up a metric to measure how accurate our model's predictions are.
# Accuracy is a common metric used to evaluate classification models like ours.
# It tells us the percentage of predictions that the model gets correct out of all predictions made.
metric = evaluate.load("accuracy")
#
# Function to compute evaluation metrics (accuracy)
def compute_metrics(eval_pred):
    '''
    This function computes accuracy metric based on evaluation predictions (logits and labels).
    It compares the predicted labels generated by the model (logits) with the actual labels (references)
    to calculate how accurate the model's predictions are.

    Parameters:
        eval_pred (tuple): A tuple containing the model's predictions (logits) and the actual labels (references).

    Returns:
        float: The computed accuracy metric, indicating the percentage of correct predictions made by the model.
    '''
    # Unpack the evaluation predictions tuple
    logits, labels = eval_pred

    # Determine the predicted labels by selecting the index with the highest probability
    predictions = np.argmax(logits, axis=-1)

    # Compute the accuracy metric by comparing the predicted labels with the actual labels
    accuracy = metric.compute(predictions=predictions, references=labels)

    # Return the computed accuracy metric
    return accuracy

# Configuration for training the model
training_args = TrainingArguments(
    # This specifies the directory where the trained model and associated files will be saved.
    # After training, all the necessary files related to the model, such as weights and configurations,
    # will be stored in this directory.
    output_dir="test_trainer",

    # This parameter determines how many training examples will be processed simultaneously
    # on each device (like GPU or CPU). By setting it to 1, we process one example at a time per device.
    #  Smaller batch sizes are often chosen to conserve memory and facilitate more frequent updates
    #  of the model's parameters during training.
    per_device_train_batch_size = 1,  # Batch size for training

    # Similar to per_device_train_batch_size, this parameter determines the batch size used during evaluation (testing)
    # of the model. Again, it's set to 1 here, indicating one example processed at a time during evaluation.
    per_device_eval_batch_size = 1,  # Batch size for evaluation

    # Gradient accumulation is a technique used to simulate larger batch sizes without increasing memory requirements.
    # Instead of updating the model's parameters after processing each batch, gradients are accumulated over multiple
    # batches before performing a parameter update. Here, we accumulate gradients over 4 batches before updating
    # the parameters. This helps stabilize training, especially when memory resources are limited.
    gradient_accumulation_steps = 4  # Gradient accumulation to simulate larger batch sizes
)

# Initialize Trainer object to facilitate training
# This line sets up a Trainer object, which will manage the training process for our model.
# Think of the Trainer as a coach that oversees the training sessions and evaluates the model's performance.
# We provide the Trainer with essential components such as the model, training arguments, datasets, and metrics.
# With this setup, the Trainer can efficiently train the model, evaluate its performance, and provide feedback.
trainer = Trainer(
    model=model,  # The model we want to train
    args=training_args,  # Configuration for training (batch size, output directory, etc.)
    train_dataset=small_train_dataset,  # Dataset used for training
    eval_dataset=small_eval_dataset,  # Dataset used for evaluation (testing)
    compute_metrics=compute_metrics,  # Function to compute evaluation metrics
)

# Evaluate the performance of the pre-trained model on the evaluation dataset
# Here, we're using the Trainer object to evaluate the performance of the pre-trained model
# on the evaluation dataset (small_eval_dataset) before fine-tuning it.
# Think of this step as checking how well the model performs right out of the box,
# without any additional training or adjustments.
# The evaluate method computes evaluation metrics, such as accuracy, using the provided evaluation dataset.
pretrained_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)
print("Metrics before fine-tuning:", pretrained_metrics)

# Train the model
# This part trains the model using the Trainer object.
# Training typically involves optimizing the model parameters based on a training dataset.
trainer.train()

# Evaluate the fine-tuned model
# Here, we evaluate the performance of the fine-tuned model.
trainer.evaluate()

# Save the trained model to a specified directory
# After training and evaluating the model, we save it to a specified directory for later use.
# Saving the model is important as it allows us to reuse it without having to train it again,
# saving time and computational resources.
output_dir = "./saved_model"
trainer.save_model(output_dir)

# Evaluate the performance of the fine-tuned model on the evaluation dataset
fine_tuned_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)
print("Metrics after fine-tuning:", fine_tuned_metrics)